{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd50f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\haraz\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\haraz\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: click in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\haraz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haraz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\haraz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User: I’ve been sneezing a lot today and my no...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User: I’ve developed a rash after eating some ...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User: My eyes are swollen and itchy, and I can...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User: I’ve been getting headaches and a stuffy...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User: Every time I eat nuts, my mouth itches. ...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       conversations  disease\n",
       "0  User: I’ve been sneezing a lot today and my no...  allergy\n",
       "1  User: I’ve developed a rash after eating some ...  allergy\n",
       "2  User: My eyes are swollen and itchy, and I can...  allergy\n",
       "3  User: I’ve been getting headaches and a stuffy...  allergy\n",
       "4  User: Every time I eat nuts, my mouth itches. ...  allergy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas nltk\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (first-time use)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\path\")\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['conversations', 'disease']].dropna()\n",
    "\n",
    "# Optional: display a few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6664de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean HTML separators and text formatting issues\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'</s>', ' ', text)         # remove </s>\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)       # remove punctuation\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['cleaned'] = df['conversations'].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3663290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\haraz\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\haraz\\anaconda3\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43fcbab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user ive been sneezing a lot today and my nose...</td>\n",
       "      <td>[s, ##neezing, nose, cong, ##ested, bot, all, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user ive developed a rash after eating some st...</td>\n",
       "      <td>[rash, eating, strawberries, bot, allergic rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user my eyes are swollen and itchy and i cant ...</td>\n",
       "      <td>[eyes, swollen, it, ##chy, s, ##neezing, bot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user ive been getting headaches and a stuffy n...</td>\n",
       "      <td>[headaches, stuffy nose, a few days, bot, alle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user every time i eat nuts my mouth itches   b...</td>\n",
       "      <td>[every, time, eat, nuts, mouth, it, ##ches, bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cleaned  \\\n",
       "0  user ive been sneezing a lot today and my nose...   \n",
       "1  user ive developed a rash after eating some st...   \n",
       "2  user my eyes are swollen and itchy and i cant ...   \n",
       "3  user ive been getting headaches and a stuffy n...   \n",
       "4  user every time i eat nuts my mouth itches   b...   \n",
       "\n",
       "                                            entities  \n",
       "0  [s, ##neezing, nose, cong, ##ested, bot, all, ...  \n",
       "1  [rash, eating, strawberries, bot, allergic rea...  \n",
       "2  [eyes, swollen, it, ##chy, s, ##neezing, bot, ...  \n",
       "3  [headaches, stuffy nose, a few days, bot, alle...  \n",
       "4  [every, time, eat, nuts, mouth, it, ##ches, bo...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create NER pipeline\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_medical_entities(text):\n",
    "    try:\n",
    "        results = nlp_ner(text)\n",
    "        return [entity['word'] for entity in results]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply to your cleaned text column\n",
    "df['entities'] = df['cleaned'].apply(extract_medical_entities)\n",
    "\n",
    "# View some results\n",
    "df[['cleaned', 'entities']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "595b8fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  veins | varicose | fever | malaria | yes | long | chills | swollen | legs | does\n",
      "Topic 2:  pain | help | chest | doctor | ill | swelling | foods | stomach | eating | try\n",
      "Topic 3:  asthma | rash | eyes | using | acne | fever | joints | yes | nosebleeds | nose\n",
      "Topic 4:  typhoid | blood | sugar | diarrhea | diabetes | stomach | ive | pressure | feeling | doctor\n",
      "Topic 5:  itchy | urine | blisters | patches | skin | yes | fever | psoriasis | theyre | yellow\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned'])\n",
    "\n",
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_topics = lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Show LDA topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}: \", \" | \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(lda, tfidf_vectorizer.get_feature_names_out(), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8395d72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haraz\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\haraz\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\haraz\\anaconda3\\lib\\subprocess.py\", line 505, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\haraz\\anaconda3\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\haraz\\anaconda3\\lib\\subprocess.py\", line 1420, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.040066299207263935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Evaluate with silhouette score\n",
    "sil_score = silhouette_score(tfidf_matrix, df['cluster'])\n",
    "print(\"Silhouette Score:\", sil_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5ecd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\haraz\\anaconda3\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from mlxtend) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from mlxtend) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from mlxtend) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from mlxtend) (3.5.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275951d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(##chy)</td>\n",
       "      <td>(bot)</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.054945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(bot)</td>\n",
       "      <td>(##chy)</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>0.074725</td>\n",
       "      <td>1.054945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(##chy)</td>\n",
       "      <td>(it)</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.637681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(it)</td>\n",
       "      <td>(##chy)</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>0.328502</td>\n",
       "      <td>4.637681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(##chy)</td>\n",
       "      <td>(yes)</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>1.301471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>(worse, yes)</td>\n",
       "      <td>(user, bot)</td>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.469027</td>\n",
       "      <td>1.786768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>(user)</td>\n",
       "      <td>(worse, bot, yes)</td>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.207031</td>\n",
       "      <td>1.774554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>(bot)</td>\n",
       "      <td>(user, worse, yes)</td>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.058242</td>\n",
       "      <td>1.054945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>(worse)</td>\n",
       "      <td>(user, bot, yes)</td>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>1.528846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>(yes)</td>\n",
       "      <td>(user, bot, worse)</td>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.082812</td>\n",
       "      <td>1.303279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>796 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      antecedents         consequents   support  confidence      lift\n",
       "0         (##chy)               (bot)  0.070833    1.000000  1.054945\n",
       "1           (bot)             (##chy)  0.070833    0.074725  1.054945\n",
       "2         (##chy)                (it)  0.070833    1.000000  4.637681\n",
       "3            (it)             (##chy)  0.070833    0.328502  4.637681\n",
       "4         (##chy)               (yes)  0.061458    0.867647  1.301471\n",
       "..            ...                 ...       ...         ...       ...\n",
       "791  (worse, yes)         (user, bot)  0.055208    0.469027  1.786768\n",
       "792        (user)   (worse, bot, yes)  0.055208    0.207031  1.774554\n",
       "793         (bot)  (user, worse, yes)  0.055208    0.058242  1.054945\n",
       "794       (worse)    (user, bot, yes)  0.055208    0.331250  1.528846\n",
       "795         (yes)  (user, bot, worse)  0.055208    0.082812  1.303279\n",
       "\n",
       "[796 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Remove empty lists\n",
    "transactions = df['entities'].dropna().apply(lambda x: list(set(x)))  # remove duplicates\n",
    "\n",
    "# One-hot encoding of transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "symptom_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apriori algorithm\n",
    "frequent_itemsets = apriori(symptom_df, min_support=0.05, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a2c86ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\haraz\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haraz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2022.9.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7d3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "df['sentiment_score'] = df['cleaned'].apply(lambda x: vader.polarity_scores(x)['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfc315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=None)\n",
    "\n",
    "# Apply to a sample (you can batch this for speed)\n",
    "df['emotions'] = df['cleaned'].apply(lambda x: emotion_classifier(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae56f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Coherence Score: 0.32300021680170954\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize text\n",
    "df['tokens'] = df['cleaned'].apply(word_tokenize)\n",
    "dictionary = Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Convert sklearn LDA to gensim\n",
    "import gensim.models.ldamodel as gldamodel\n",
    "gensim_lda = gldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=5,\n",
    "    passes=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "coherence_model = CoherenceModel(model=gensim_lda, texts=df['tokens'], dictionary=dictionary, coherence='c_v')\n",
    "print(\"LDA Coherence Score:\", coherence_model.get_coherence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55f36147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score for Clustering: 0.040066299207263935\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Score for Clustering:\", silhouette_score(tfidf_matrix, df['cluster']))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
